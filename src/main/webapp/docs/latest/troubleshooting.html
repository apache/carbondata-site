<!--
    Licensed to the Apache Software Foundation (ASF) under one
    or more contributor license agreements.  See the NOTICE file
    distributed with this work for additional information
    regarding copyright ownership.  The ASF licenses this file
    to you under the Apache License, Version 2.0 (the
    "License"); you may not use this file except in compliance
    with the License.  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing,
    software distributed under the License is distributed on an
    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
    KIND, either express or implied.  See the License for the
    specific language governing permissions and limitations
    under the License.
--><h1>Troubleshooting</h1><p>This tutorial is designed to provide troubleshooting for end users and
    developers who are building, deploying, and using CarbonData.</p><h2>Failed to load thrift
    libraries</h2><p><strong>Symptom</strong></p><p>Thrift throws following exception :</p><p><code>
    thrift: error while loading shared libraries:
    libthriftc.so.0: cannot open shared object file: No such file or directory
</code></p><p><strong>Possible Cause</strong></p><p>The complete path to the directory containing
    the libraries is not configured correctly.</p><p><strong>Procedure</strong></p><p>Follow the
    steps below to ensure loading of libraries appropriately :</p>
<ol>
    <li><p>For ubuntu you have to add a custom.conf file to /etc/ld.so.conf.d For example,</p>
        <p><code>
            sudo gedit /etc/ld.so.conf.d/randomLibs.conf
        </code></p>
        <p>Inside this file you are supposed to configure the complete path to the directory that
            contains all the libraries that you wish to add to the system, let us say
            /home/ubuntu/localLibs</p></li>
    <li><p>To ensure your library location ,check for existence of libthrift.so</p></li>
    <li><p>Save and run the following command to update the system with this libs.</p>
        <p><code>
            sudo ldconfig
        </code></p>
        <p>Note : Remember to add only the path to the directory, not the full path for that file,
            all the libraries inside that path will be automatically indexed.</p></li>
</ol><h2>Failed to launch the Spark Shell</h2><p><strong>Symptom</strong></p><p>The shell prompts
    the following error :</p><p><code>
    org.apache.spark.sql.CarbonContext$$anon$$apache$spark$sql$catalyst$analysis
    $OverrideCatalog$_setter_$org$apache$spark$sql$catalyst$analysis
    $OverrideCatalog$$overrides_$e
</code></p><p><strong>Possible Cause</strong></p><p>The Spark Version and the selected Spark Profile
    do not match.</p><p><strong>Procedure</strong></p>
<ol>
    <li><p>Ensure your spark version and selected profile for spark are correct.</p></li>
    <li><p>Use the following command :</p>
        <p><code>
            &quot;mvn -Pspark-2.1 -Dspark.version {yourSparkVersion} clean package&quot;
        </code></p>
        <p>Note : Refrain from using "mvn clean package" without specifying the profile.</p></li>
</ol><h2>Failed to execute load query on cluster.</h2><p><strong>Symptom</strong></p><p>Load query
    failed with the following exception:</p><p><code>
    Dictionary file is locked for updation.
</code></p><p><strong>Possible Cause</strong></p><p>The carbon.properties file is not identical in
    all the nodes of the cluster.</p><p><strong>Procedure</strong></p><p>Follow the steps to ensure
    the carbon.properties file is consistent across all the nodes:</p>
<ol>
    <li><p>Copy the carbon.properties file from the master node to all the other nodes in the
        cluster. For example, you can use ssh to copy this file to all the nodes.</p></li>
    <li><p>For the changes to take effect, restart the Spark cluster.</p></li>
</ol><h2>Failed to execute insert query on cluster.</h2><p><strong>Symptom</strong></p><p>Load query
    failed with the following exception:</p><p><code>
    Dictionary file is locked for updation.
</code></p><p><strong>Possible Cause</strong></p><p>The carbon.properties file is not identical in
    all the nodes of the cluster.</p><p><strong>Procedure</strong></p><p>Follow the steps to ensure
    the carbon.properties file is consistent across all the nodes:</p>
<ol>
    <li><p>Copy the carbon.properties file from the master node to all the other nodes in the
        cluster. For example, you can use scp to copy this file to all the nodes.</p></li>
    <li><p>For the changes to take effect, restart the Spark cluster.</p></li>
</ol><h2>Failed to connect to hiveuser with thrift</h2><p><strong>Symptom</strong></p><p>We get the
    following exception :</p><p><code>
    Cannot connect to hiveuser.
</code></p><p><strong>Possible Cause</strong></p><p>The external process does not have permission to
    access.</p><p><strong>Procedure</strong></p><p>Ensure that the Hiveuser in mysql must allow its
    access to the external processes.</p><h2>Failure to read the metastore db during table
    creation.</h2><p><strong>Symptom</strong></p><p>We get the following exception on trying to
    connect :</p><p><code>
    Cannot read the metastore db
</code></p><p><strong>Possible Cause</strong></p><p>The metastore db is dysfunctional.</p><p>
    <strong>Procedure</strong></p><p>Remove the metastore db from the carbon.metastore in the Spark
    Directory.</p><h2>Failed to load data on the cluster</h2><p><strong>Symptom</strong></p><p>Data
    loading fails with the following exception :</p><p><code>
    Data Load failure exeception
</code></p><p><strong>Possible Cause</strong></p><p>The following issue can cause the failure :</p>
<ol>
    <li><p>The core-site.xml, hive-site.xml, yarn-site and carbon.properties are not consistent
        across all nodes of the cluster.</p></li>
    <li><p>Path to hdfs ddl is not configured correctly in the carbon.properties.</p></li>
</ol><p><strong>Procedure</strong></p><p>Follow the steps to ensure the following configuration
    files are consistent across all the nodes:</p>
<ol>
    <li><p>Copy the core-site.xml, hive-site.xml, yarn-site,carbon.properties files from the master
        node to all the other nodes in the cluster. For example, you can use scp to copy this file
        to all the nodes.</p>
        <p>Note : Set the path to hdfs ddl in carbon.properties in the master node.</p></li>
    <li><p>For the changes to take effect, restart the Spark cluster.</p></li>
</ol><h2>Failed to insert data on the cluster</h2><p><strong>Symptom</strong></p><p>Insertion fails
    with the following exception :</p><p><code>
    Data Load failure exeception
</code></p><p><strong>Possible Cause</strong></p><p>The following issue can cause the failure :</p>
<ol>
    <li><p>The core-site.xml, hive-site.xml, yarn-site and carbon.properties are not consistent
        across all nodes of the cluster.</p></li>
    <li><p>Path to hdfs ddl is not configured correctly in the carbon.properties.</p></li>
</ol><p><strong>Procedure</strong></p><p>Follow the steps to ensure the following configuration
    files are consistent across all the nodes:</p>
<ol>
    <li><p>Copy the core-site.xml, hive-site.xml, yarn-site,carbon.properties files from the master
        node to all the other nodes in the cluster. For example, you can use scp to copy this file
        to all the nodes.</p>
        <p>Note : Set the path to hdfs ddl in carbon.properties in the master node.</p></li>
    <li><p>For the changes to take effect, restart the Spark cluster.</p></li>
</ol><h2>Failed to execute Concurrent Operations(Load,Insert,Update) on table by multiple
    workers.</h2><p><strong>Symptom</strong></p><p>Execution fails with the following exception
    :</p><p><code>
    Table is locked for updation.
</code></p><p><strong>Possible Cause</strong></p><p>Concurrency not supported.</p><p><strong>Procedure</strong>
</p><p>Worker must wait for the query execution to complete and the table to release the lock for
    another query execution to succeed..</p><h2>Failed to create a table with a single numeric
    column.</h2><p><strong>Symptom</strong></p><p>Execution fails with the following exception :</p>
<p><code>
    Table creation fails.
</code></p><p><strong>Possible Cause</strong></p><p>Behavior not supported.</p><p>
    <strong>Procedure</strong></p><p>A single column that can be considered as dimension is
    mandatory for table creation.</p>